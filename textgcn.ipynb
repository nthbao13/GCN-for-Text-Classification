{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dbe4b4-dcdd-4783-834e-4a875a0938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 3. Load data\n",
    "df = pd.read_csv('train.csv', skiprows=1, header=None)\n",
    "df.columns = ['label', 'title', 'description']\n",
    "df['content'] = df['title'] + ' ' + df['description']\n",
    "df = df.head(120000)  \n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "labels = torch.tensor(df['label'].values) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894ab91d-664f-4e64-9698-8ec74adfedb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading existing embeddings...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 4. Load Sentence-BERT\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Model nhẹ, nhanh\n",
    "\n",
    "# 5. Create document embeddings\n",
    "embedding_file = 'doc_embeddings_sbert.pt'\n",
    "if os.path.exists(embedding_file):\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    doc_embeddings = torch.load(embedding_file)\n",
    "    if not isinstance(doc_embeddings, torch.Tensor):\n",
    "        print(f\"Error: doc_embeddings is {type(doc_embeddings)}, expected torch.Tensor. Regenerating embeddings...\")\n",
    "        os.remove(embedding_file)\n",
    "        sentences = df['content'].tolist()\n",
    "        embeddings = sbert_model.encode(sentences, show_progress_bar=True, convert_to_tensor=True)\n",
    "        doc_embeddings = embeddings.cpu()\n",
    "        torch.save(doc_embeddings, embedding_file)\n",
    "        print(f\"Saved SBERT embeddings to {embedding_file}\")\n",
    "else:\n",
    "    print(\"Generating new embeddings with Sentence-BERT...\")\n",
    "    sentences = df['content'].tolist()\n",
    "    embeddings = sbert_model.encode(sentences, show_progress_bar=True, convert_to_tensor=True)\n",
    "    doc_embeddings = embeddings.cpu()\n",
    "    torch.save(doc_embeddings, embedding_file)\n",
    "    print(f\"Saved SBERT embeddings to {embedding_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c8723b-e71e-461b-a2be-bcc6c970fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build vocabulary\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "X_counts = vectorizer.fit_transform(df['content'])\n",
    "word_vocab = vectorizer.get_feature_names_out()\n",
    "word2idx = {word: idx for idx, word in enumerate(word_vocab)}\n",
    "\n",
    "# 7. Create word embeddings (using random init)\n",
    "word_embeddings = torch.randn(len(word_vocab), doc_embeddings.size(1))  # phải cùng chiều với doc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7f425f-b2ea-42d7-9423-1166df177339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create edges\n",
    "tfidf = TfidfVectorizer(vocabulary=word_vocab)\n",
    "X_tfidf = tfidf.fit_transform(df['content'])\n",
    "\n",
    "# Word-to-Document edges (w2d) & Document-to-Word edges (d2w)\n",
    "row, col, edge_weight = [], [], []\n",
    "doc_offset = len(word_vocab)\n",
    "\n",
    "for doc_idx, row_data in enumerate(X_tfidf):\n",
    "    non_zero_indices = row_data.nonzero()[1]\n",
    "    for word_idx in non_zero_indices:\n",
    "        row.append(word_idx)\n",
    "        col.append(doc_idx + doc_offset)\n",
    "        edge_weight.append(X_tfidf[doc_idx, word_idx])\n",
    "        row.append(doc_idx + doc_offset)\n",
    "        col.append(word_idx)\n",
    "        edge_weight.append(X_tfidf[doc_idx, word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c70eb5-870d-48aa-84a9-d4569e7208cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges created: 13480136\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "window_size = 20\n",
    "\n",
    "# Word-to-Word edges (w2w) using PMI\n",
    "word_count = defaultdict(int)\n",
    "co_occur = defaultdict(int)\n",
    "total_words = 0\n",
    "\n",
    "# Tính PMI giữa các từ\n",
    "def calculate_pmi(word_count, co_occur, total_words):\n",
    "    pmi_matrix = {}\n",
    "    for (word1, word2), count in co_occur.items():\n",
    "        # Tính xác suất đồng xuất hiện P(w1, w2)\n",
    "        p_w1_w2 = count / total_words\n",
    "        # Tính xác suất xuất hiện của từng từ P(w1) và P(w2)\n",
    "        p_w1 = word_count[word1] / total_words\n",
    "        p_w2 = word_count[word2] / total_words\n",
    "        # Tính PMI\n",
    "        pmi = math.log(p_w1_w2 / (p_w1 * p_w2) + 1e-8)  # +1e-8 để tránh log(0)\n",
    "        pmi_matrix[(word1, word2)] = pmi\n",
    "    return pmi_matrix\n",
    "\n",
    "for text in df['content']:\n",
    "    words = text.split()\n",
    "    total_words += len(words)\n",
    "    for i, word in enumerate(words):\n",
    "        word_count[word] += 1\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                co_occur[(word, words[j])] += 1\n",
    "\n",
    "pmi_matrix = calculate_pmi(word_count, co_occur, total_words)\n",
    "\n",
    "w2w_threshold = 0.2\n",
    "for (word1, word2), pmi in pmi_matrix.items():\n",
    "    if pmi > w2w_threshold:\n",
    "        if word1 in word2idx and word2 in word2idx:\n",
    "            row.append(word2idx[word1])  # Chuyển từ thành chỉ mục\n",
    "            col.append(word2idx[word2])\n",
    "            edge_weight.append(pmi)\n",
    "            row.append(word2idx[word2])\n",
    "            col.append(word2idx[word1])\n",
    "            edge_weight.append(pmi)\n",
    "\n",
    "print(f\"Number of edges created: {len(row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "271c61ab-83a9-429d-b97d-c979b43d3d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xây dựng cạnh Document-to-Document với phương pháp cân bằng...\n",
      "Đang xây dựng FAISS index cho 120000 tài liệu...\n",
      "Đang xác định ngưỡng tương đồng phù hợp...\n",
      "Ngưỡng tương đồng được xác định: 0.7000\n",
      "Đang tải kết quả tìm kiếm láng giềng từ cache...\n",
      "Đang thêm cạnh với ngưỡng 0.7000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 120000/120000 [00:06<00:00, 17412.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoàn thành! Đã tạo 415647 cặp cạnh D2D\n",
      "Tổng số cạnh hiện tại: 14311430\n",
      "Số cạnh trung bình mỗi tài liệu: 6.93\n"
     ]
    }
   ],
   "source": [
    "# Thêm thư viện cần thiết\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "print(\"Đang xây dựng cạnh Document-to-Document với phương pháp cân bằng...\")\n",
    "\n",
    "# Chuyển document embeddings sang numpy để xử lý\n",
    "doc_embeddings_np = doc_embeddings.cpu().numpy()\n",
    "doc_count = doc_embeddings_np.shape[0]\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "gc.collect()\n",
    "\n",
    "# Thông số cấu hình\n",
    "initial_threshold = 0.3  # Bắt đầu với ngưỡng thấp hơn\n",
    "max_threshold = 0.7      # Ngưỡng tối đa nếu cần điều chỉnh\n",
    "target_edges_per_node = 5  # Mục tiêu số lượng cạnh trung bình cho mỗi node\n",
    "k = min(50, doc_count)  # Số láng giềng gần nhất cần tìm\n",
    "\n",
    "# Chuẩn hóa embedding để sử dụng dot product cho cosine similarity\n",
    "norms = np.linalg.norm(doc_embeddings_np, axis=1, keepdims=True)\n",
    "normalized_embeddings = doc_embeddings_np / norms\n",
    "d = normalized_embeddings.shape[1]  # Số chiều của embedding\n",
    "\n",
    "print(f\"Đang xây dựng FAISS index cho {doc_count} tài liệu...\")\n",
    "\n",
    "# Khởi tạo FAISS index (Inner Product cho vector đã chuẩn hóa tương đương cosine similarity)\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(normalized_embeddings.astype('float32'))\n",
    "\n",
    "# Thử nghiệm với một tập nhỏ để xác định ngưỡng phù hợp\n",
    "print(\"Đang xác định ngưỡng tương đồng phù hợp...\")\n",
    "sample_size = min(1000, doc_count)\n",
    "sample_indices = np.random.choice(doc_count, sample_size, replace=False)\n",
    "sample_embeddings = normalized_embeddings[sample_indices].astype('float32')\n",
    "\n",
    "# Tìm kiếm k láng giềng gần nhất cho mẫu\n",
    "sims, neighbors = index.search(sample_embeddings, k)\n",
    "\n",
    "# Thống kê phân bố độ tương đồng để xác định ngưỡng\n",
    "all_similarities = []\n",
    "for i in range(sample_size):\n",
    "    # Bỏ qua phần tử đầu tiên (chính nó)\n",
    "    all_similarities.extend(sims[i][1:].tolist())\n",
    "\n",
    "all_similarities = sorted(all_similarities, reverse=True)\n",
    "total_possible_edges = len(all_similarities)\n",
    "\n",
    "# Xác định ngưỡng dựa trên mật độ mục tiêu\n",
    "target_total_edges = doc_count * target_edges_per_node\n",
    "target_density = min(1.0, target_total_edges / (doc_count * (doc_count - 1) / 2))\n",
    "edge_index = min(int(total_possible_edges * target_density * (sample_size / doc_count)), len(all_similarities) - 1)\n",
    "adaptive_threshold = max(initial_threshold, all_similarities[edge_index])\n",
    "adaptive_threshold = min(adaptive_threshold, max_threshold)  # Giới hạn trên\n",
    "\n",
    "print(f\"Ngưỡng tương đồng được xác định: {adaptive_threshold:.4f}\")\n",
    "\n",
    "# Lưu cache kết quả tìm kiếm để tránh tính toán lại\n",
    "cache_file = 'faiss_neighbors_cache.npz'\n",
    "use_cache = False\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    print(f\"Đang tải kết quả tìm kiếm láng giềng từ cache...\")\n",
    "    cache = np.load(cache_file, allow_pickle=True)\n",
    "    all_neighbors = cache['neighbors']\n",
    "    all_sims = cache['sims']\n",
    "    use_cache = True\n",
    "else:\n",
    "    # Thêm cạnh D2D theo batch để tiết kiệm bộ nhớ\n",
    "    batch_size = 1000\n",
    "    all_neighbors = []\n",
    "    all_sims = []\n",
    "\n",
    "    print(f\"Đang tìm kiếm {k} láng giềng gần nhất cho mỗi tài liệu...\")\n",
    "    \n",
    "    for i in tqdm(range(0, doc_count, batch_size)):\n",
    "        end_idx = min(i + batch_size, doc_count)\n",
    "        batch = normalized_embeddings[i:end_idx].astype('float32')\n",
    "        \n",
    "        # Tìm kiếm k láng giềng gần nhất\n",
    "        batch_sims, batch_neighbors = index.search(batch, k)\n",
    "        \n",
    "        # Lưu kết quả\n",
    "        all_neighbors.extend(batch_neighbors.tolist())\n",
    "        all_sims.extend(batch_sims.tolist())\n",
    "        \n",
    "        # Giải phóng bộ nhớ\n",
    "        del batch_sims, batch_neighbors\n",
    "        gc.collect()\n",
    "    \n",
    "    # Lưu cache để sử dụng sau\n",
    "    np.savez_compressed(cache_file, neighbors=np.array(all_neighbors), sims=np.array(all_sims))\n",
    "\n",
    "# Xây dựng cạnh document-to-document với ngưỡng đã điều chỉnh\n",
    "d2d_edges_count = 0\n",
    "print(f\"Đang thêm cạnh với ngưỡng {adaptive_threshold:.4f}...\")\n",
    "\n",
    "for doc_idx in tqdm(range(doc_count)):\n",
    "    neighbors = all_neighbors[doc_idx]\n",
    "    sims = all_sims[doc_idx]\n",
    "    \n",
    "    for j in range(1, len(neighbors)):  # Bỏ qua chính nó (j=0)\n",
    "        neighbor_idx = neighbors[j]\n",
    "        sim_value = sims[j]\n",
    "        \n",
    "        # Chỉ thêm cạnh cho cặp có chỉ số (i<j) để tránh trùng lặp\n",
    "        # và chỉ khi độ tương đồng vượt ngưỡng\n",
    "        if sim_value > adaptive_threshold and doc_idx < neighbor_idx:\n",
    "            row.append(doc_idx + doc_offset)\n",
    "            col.append(neighbor_idx + doc_offset)\n",
    "            edge_weight.append(float(sim_value))\n",
    "            \n",
    "            # Thêm cạnh ngược lại (vì đồ thị vô hướng)\n",
    "            row.append(neighbor_idx + doc_offset)\n",
    "            col.append(doc_idx + doc_offset)\n",
    "            edge_weight.append(float(sim_value))\n",
    "            \n",
    "            d2d_edges_count += 1\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "del normalized_embeddings, index, doc_embeddings_np\n",
    "if use_cache:\n",
    "    del all_neighbors, all_sims\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Hoàn thành! Đã tạo {d2d_edges_count} cặp cạnh D2D\")\n",
    "print(f\"Tổng số cạnh hiện tại: {len(row)}\")\n",
    "print(f\"Số cạnh trung bình mỗi tài liệu: {d2d_edges_count*2/doc_count:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7db284-f248-4a75-a80e-4b0c87bf640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train nodes: 84000\n",
      "Validation nodes: 18000\n",
      "Test nodes: 18000\n"
     ]
    }
   ],
   "source": [
    "# 7. Create graph data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Chia dữ liệu thành train, validation, test\n",
    "num_docs = doc_embeddings.size(0)\n",
    "indices = np.arange(num_docs)\n",
    "train_idx, temp_idx = train_test_split(indices, train_size=0.7, random_state=42, stratify=df['label'])\n",
    "val_idx, test_idx = train_test_split(temp_idx, train_size=0.5, random_state=42, stratify=df['label'].iloc[temp_idx])\n",
    "\n",
    "# Tạo masks\n",
    "num_words = word_embeddings.size(0)\n",
    "train_mask = torch.zeros(num_words + num_docs, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_words + num_docs, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_words + num_docs, dtype=torch.bool)\n",
    "\n",
    "# Chỉ các node tài liệu được sử dụng\n",
    "train_mask[num_words + train_idx] = True\n",
    "val_mask[num_words + val_idx] = True\n",
    "test_mask[num_words + test_idx] = True\n",
    "\n",
    "# Kiểm tra số lượng node trong mỗi tập\n",
    "print(f\"Train nodes: {train_mask[num_words:].sum().item()}\")\n",
    "print(f\"Validation nodes: {val_mask[num_words:].sum().item()}\")\n",
    "print(f\"Test nodes: {test_mask[num_words:].sum().item()}\")\n",
    "\n",
    "# Tạo labels với kích thước num_words + num_docs\n",
    "labels = torch.full((num_words + num_docs,), -1, dtype=torch.long)  # -1 cho node không có nhãn\n",
    "labels[num_words:num_words + num_docs] = torch.tensor(df['label'].astype(int).values) - 1  # Nhãn cho node tài liệu\n",
    "\n",
    "# Tạo edge_index, edge_attr\n",
    "edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "edge_attr = torch.tensor(edge_weight, dtype=torch.float)\n",
    "\n",
    "# Tạo đối tượng Data\n",
    "data = Data(x=torch.cat([word_embeddings, doc_embeddings], dim=0),\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=labels,\n",
    "            train_mask=train_mask,\n",
    "            val_mask=val_mask,\n",
    "            test_mask=test_mask)\n",
    "\n",
    "# 8. Define GCN model\n",
    "class TextGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(TextGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        return x\n",
    "\n",
    "# 9. Train model with early stopping\n",
    "input_dim = doc_embeddings.size(1)  # Thường là 384 với SBERT Mini\n",
    "word_embeddings = torch.randn(len(word_vocab), input_dim)\n",
    "x = torch.cat([word_embeddings, doc_embeddings], dim=0)\n",
    "\n",
    "data = Data(x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=labels,\n",
    "            train_mask=train_mask,\n",
    "            val_mask=val_mask,\n",
    "            test_mask=test_mask)\n",
    "\n",
    "model = TextGCN(input_dim, 128, 4).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# Sử dụng ignore_index=-1 để bỏ qua node không có nhãn\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# Thêm early stopping\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "max_epochs = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(max_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "    # Tính loss chỉ trên các node trong train_mask\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Đánh giá trên tập validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "        val_preds = logits[data.val_mask].argmax(dim=1)\n",
    "        val_acc = (val_preds == data.y[data.val_mask].to(device)).float().mean().item()\n",
    "\n",
    "    model.train()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Tải mô hình tốt nhất\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# 10. Evaluate on all sets\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "    train_preds = logits[data.train_mask].argmax(dim=1)\n",
    "    train_acc = (train_preds == data.y[data.train_mask].to(device)).float().mean().item()\n",
    "    val_preds = logits[data.val_mask].argmax(dim=1)\n",
    "    val_acc = (val_preds == data.y[data.val_mask].to(device)).float().mean().item()\n",
    "    test_preds = logits[data.test_mask].argmax(dim=1)\n",
    "    test_acc = (test_preds == data.y[data.test_mask].to(device)).float().mean().item()\n",
    "\n",
    "print(f'Training Accuracy: {train_acc:.4f}')\n",
    "print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# 11. Save model\n",
    "torch.save(model.state_dict(), 'textgcn_model.pth')\n",
    "\n",
    "# 12. Load model (nếu cần)\n",
    "# model.load_state_dict(torch.load('textgcn_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
