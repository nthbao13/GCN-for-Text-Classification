{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbe4b4-dcdd-4783-834e-4a875a0938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 3. Load data\n",
    "df = pd.read_csv('train.csv', skiprows=1, header=None)\n",
    "df.columns = ['label', 'title', 'description']\n",
    "df['content'] = df['title'] + ' ' + df['description']\n",
    "df = df.head(120000)  \n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "labels = torch.tensor(df['label'].values) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ab91d-664f-4e64-9698-8ec74adfedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 4. Load Sentence-BERT\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Model nhẹ, nhanh\n",
    "\n",
    "# 5. Create document embeddings\n",
    "embedding_file = 'doc_embeddings_sbert.pt'\n",
    "if os.path.exists(embedding_file):\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    doc_embeddings = torch.load(embedding_file)\n",
    "    if not isinstance(doc_embeddings, torch.Tensor):\n",
    "        print(f\"Error: doc_embeddings is {type(doc_embeddings)}, expected torch.Tensor. Regenerating embeddings...\")\n",
    "        os.remove(embedding_file)\n",
    "        sentences = df['content'].tolist()\n",
    "        embeddings = sbert_model.encode(sentences, show_progress_bar=True, convert_to_tensor=True)\n",
    "        doc_embeddings = embeddings.cpu()\n",
    "        torch.save(doc_embeddings, embedding_file)\n",
    "        print(f\"Saved SBERT embeddings to {embedding_file}\")\n",
    "else:\n",
    "    print(\"Generating new embeddings with Sentence-BERT...\")\n",
    "    sentences = df['content'].tolist()\n",
    "    embeddings = sbert_model.encode(sentences, show_progress_bar=True, convert_to_tensor=True)\n",
    "    doc_embeddings = embeddings.cpu()\n",
    "    torch.save(doc_embeddings, embedding_file)\n",
    "    print(f\"Saved SBERT embeddings to {embedding_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8723b-e71e-461b-a2be-bcc6c970fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build vocabulary\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "X_counts = vectorizer.fit_transform(df['content'])\n",
    "word_vocab = vectorizer.get_feature_names_out()\n",
    "word2idx = {word: idx for idx, word in enumerate(word_vocab)}\n",
    "\n",
    "# 7. Create word embeddings (using random init)\n",
    "word_embeddings = torch.randn(len(word_vocab), doc_embeddings.size(1))  # phải cùng chiều với doc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f425f-b2ea-42d7-9423-1166df177339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create edges\n",
    "tfidf = TfidfVectorizer(vocabulary=word_vocab)\n",
    "X_tfidf = tfidf.fit_transform(df['content'])\n",
    "\n",
    "# Word-to-Document edges (w2d) & Document-to-Word edges (d2w)\n",
    "row, col, edge_weight = [], [], []\n",
    "doc_offset = len(word_vocab)\n",
    "\n",
    "for doc_idx, row_data in enumerate(X_tfidf):\n",
    "    non_zero_indices = row_data.nonzero()[1]\n",
    "    for word_idx in non_zero_indices:\n",
    "        row.append(word_idx)\n",
    "        col.append(doc_idx + doc_offset)\n",
    "        edge_weight.append(X_tfidf[doc_idx, word_idx])\n",
    "        row.append(doc_idx + doc_offset)\n",
    "        col.append(word_idx)\n",
    "        edge_weight.append(X_tfidf[doc_idx, word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c70eb5-870d-48aa-84a9-d4569e7208cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-to-Word edges (w2w) using PMI\n",
    "word_count = defaultdict(int)\n",
    "co_occur = defaultdict(int)\n",
    "total_words = 0\n",
    "\n",
    "for text in df['content']:\n",
    "    words = text.split()\n",
    "    total_words += len(words)\n",
    "    for i, word in enumerate(words):\n",
    "        word_count[word] += 1\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                co_occur[(word, words[j])] += 1\n",
    "\n",
    "pmi_matrix = calculate_pmi(word_count, co_occur, total_words)\n",
    "\n",
    "w2w_threshold = 0.2\n",
    "for (word1, word2), pmi in pmi_matrix.items():\n",
    "    if pmi > w2w_threshold:\n",
    "        row.append(word2idx[word1])  # Chuyển từ thành chỉ mục\n",
    "        col.append(word2idx[word2])\n",
    "        edge_weight.append(pmi)\n",
    "        row.append(word2idx[word2])\n",
    "        col.append(word2idx[word1])\n",
    "        edge_weight.append(pmi)\n",
    "\n",
    "print(f\"Number of edges created: {len(row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c61ab-83a9-429d-b97d-c979b43d3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-to-Document edges (d2d) using document embeddings and cosine similarity\n",
    "doc_embeddings_np = doc_embeddings.cpu().numpy()\n",
    "doc_cos_sim = cosine_similarity(doc_embeddings_np)\n",
    "d2d_threshold = 0.2\n",
    "for i in range(doc_embeddings_np.shape[0]):\n",
    "    for j in range(i + 1, doc_embeddings_np.shape[0]):  # Tránh lặp lại và cạnh tự nối\n",
    "        if doc_cos_sim[i, j] > d2d_threshold:\n",
    "            row.append(i + doc_offset)\n",
    "            col.append(j + doc_offset)\n",
    "            edge_weight.append(float(doc_cos_sim[i, j]))\n",
    "            row.append(j + doc_offset)\n",
    "            col.append(i + doc_offset)\n",
    "            edge_weight.append(float(doc_cos_sim[i, j]))\n",
    "\n",
    "print(f\"Number of edges created: {len(row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7db284-f248-4a75-a80e-4b0c87bf640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3889\n",
      "Epoch 2, Loss: 1.3380\n",
      "Epoch 3, Loss: 1.2808\n",
      "Epoch 4, Loss: 1.2090\n",
      "Epoch 5, Loss: 1.1267\n",
      "Epoch 6, Loss: 1.0389\n",
      "Epoch 7, Loss: 0.9504\n",
      "Epoch 8, Loss: 0.8648\n",
      "Epoch 9, Loss: 0.7844\n",
      "Epoch 10, Loss: 0.7111\n",
      "Epoch 11, Loss: 0.6468\n",
      "Epoch 12, Loss: 0.5926\n",
      "Epoch 13, Loss: 0.5484\n",
      "Epoch 14, Loss: 0.5128\n",
      "Epoch 15, Loss: 0.4842\n",
      "Epoch 16, Loss: 0.4619\n",
      "Epoch 17, Loss: 0.4449\n",
      "Epoch 18, Loss: 0.4322\n",
      "Epoch 19, Loss: 0.4227\n",
      "Epoch 20, Loss: 0.4155\n",
      "Training Accuracy: 0.8631\n"
     ]
    }
   ],
   "source": [
    "# 7. Create graph datab\n",
    "edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "edge_attr = torch.tensor(edge_weight, dtype=torch.float)\n",
    "labels = torch.tensor(df['label'].astype(int).values) - 1  # Labels from 0 → 3\n",
    "\n",
    "# Mask cho train/test (ở đây train hết cho đơn giản)\n",
    "num_words = word_embeddings.size(0)\n",
    "num_docs = doc_embeddings.size(0)\n",
    "\n",
    "# Mask cho các node văn bản\n",
    "train_mask = torch.zeros(num_words + num_docs, dtype=torch.bool)\n",
    "train_mask[num_words:] = True  # Chỉ dùng doc nodes để train\n",
    "\n",
    "# Tạo lại data\n",
    "data = Data(x=torch.cat([word_embeddings, doc_embeddings], dim=0),\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=labels,\n",
    "            train_mask=train_mask)\n",
    "\n",
    "# 8. Define GCN model\n",
    "class TextGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(TextGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        return x\n",
    "\n",
    "# 9. Train model\n",
    "input_dim = doc_embeddings.size(1)  # Thường là 384 với SBERT Mini\n",
    "word_embeddings = torch.randn(len(word_vocab), input_dim)\n",
    "x = torch.cat([word_embeddings, doc_embeddings], dim=0)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "            y=labels, train_mask=train_mask)\n",
    "\n",
    "model = TextGCN(input_dim, 128, 4).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "    loss = criterion(out[data.train_mask], data.y.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 10. Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "    preds = logits[data.train_mask].argmax(dim=1)\n",
    "    acc = (preds == data.y.to(device)).float().mean()\n",
    "    print(f'Training Accuracy: {acc:.4f}')\n",
    "\n",
    "# 11. Save model\n",
    "torch.save(model.state_dict(), 'textgcn_model.pth')\n",
    "\n",
    "# 12. Load model\n",
    "# model.load_state_dict(torch.load('textgcn_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
